{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hate Speech Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as panda\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import *\n",
    "import string\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn\n",
    "from textstat.textstat import *\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import numpy as np\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer as VS\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset = panda.read_csv(\"HateSpeechData.csv\")\n",
    "dataset = panda.read_csv(\"labeled_data.csv\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding text-length as a field in the dataset\n",
    "dataset['text length'] = dataset['tweet'].apply(len)\n",
    "print(dataset.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "graph = sns.FacetGrid(data=dataset, col='class')\n",
    "graph.map(plt.hist, 'text length', bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box-plot visvualization\n",
    "sns.boxplot(x='class', y='text length', data=dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['class'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet=dataset.tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing of the tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1. Removal of punctuation and capitlization\n",
    "## 2. Tokenizing\n",
    "## 3. Removal of stopwords\n",
    "## 4. Stemming\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "\n",
    "#extending the stopwords to include other words used in twitter such as retweet(rt) etc.\n",
    "other_exclusions = [\"#ff\", \"ff\", \"rt\"]\n",
    "stopwords.extend(other_exclusions)\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def preprocess(tweet):  \n",
    "    \n",
    "    # removal of extra spaces\n",
    "    regex_pat = re.compile(r'\\s+')\n",
    "    tweet_space = tweet.str.replace(regex_pat, ' ')\n",
    "\n",
    "    # removal of @name[mention]\n",
    "    regex_pat = re.compile(r'@[\\w\\-]+')\n",
    "    tweet_name = tweet_space.str.replace(regex_pat, '')\n",
    "\n",
    "    # removal of links[https://abc.com]\n",
    "    giant_url_regex =  re.compile('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|'\n",
    "            '[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    tweets = tweet_name.str.replace(giant_url_regex, '')\n",
    "    \n",
    "    # removal of punctuations and numbers\n",
    "    punc_remove = tweets.str.replace(\"[^a-zA-Z]\", \" \")\n",
    "    # remove whitespace with a single space\n",
    "    newtweet=punc_remove.str.replace(r'\\s+', ' ')\n",
    "    # remove leading and trailing whitespace\n",
    "    newtweet=newtweet.str.replace(r'^\\s+|\\s+?$','')\n",
    "    # replace normal numbers with numbr\n",
    "    newtweet=newtweet.str.replace(r'\\d+(\\.\\d+)?','numbr')\n",
    "    # removal of capitalization\n",
    "    tweet_lower = newtweet.str.lower()\n",
    "    \n",
    "    # tokenizing\n",
    "    tokenized_tweet = tweet_lower.apply(lambda x: x.split())\n",
    "    \n",
    "    # removal of stopwords\n",
    "    tokenized_tweet=  tokenized_tweet.apply(lambda x: [item for item in x if item not in stopwords])\n",
    "    \n",
    "    # stemming of the tweets\n",
    "    tokenized_tweet = tokenized_tweet.apply(lambda x: [stemmer.stem(i) for i in x]) \n",
    "    \n",
    "    for i in range(len(tokenized_tweet)):\n",
    "        tokenized_tweet[i] = ' '.join(tokenized_tweet[i])\n",
    "        tweets_p= tokenized_tweet\n",
    "    \n",
    "    return tweets_p\n",
    "\n",
    "processed_tweets = preprocess(tweet)   \n",
    "\n",
    "dataset['processed_tweets'] = processed_tweets\n",
    "print(dataset[[\"tweet\",\"processed_tweets\"]].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizing which of the word is most commonly used in the twitter dataset\n",
    "from wordcloud import WordCloud\n",
    "# imshow-Display data as an image\n",
    "# interpolation - https://matplotlib.org/3.2.1/gallery/images_contours_and_fields/interpolation_methods.html\n",
    "all_words = ' '.join([text for text in dataset['processed_tweets'] ])\n",
    "wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(all_words)\n",
    "#random=0.30\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizing which of the word is most commonly used for hatred speech\n",
    "hatred_words = ' '.join([text for text in dataset['processed_tweets'][dataset['class'] == 0]])\n",
    "wordcloud = WordCloud(width=800, height=500,\n",
    "random_state=21, max_font_size=110).generate(hatred_words)\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizing which of the word is most commonly used for offensive speech\n",
    "offensive_words = ' '.join([text for text in dataset['processed_tweets'][dataset['class'] == 1]])\n",
    "wordcloud = WordCloud(width=800, height=500,\n",
    "random_state=21, max_font_size=110).generate(offensive_words)\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF-IDF Features-F1\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 2),max_df=0.75, min_df=5, max_features=10000)\n",
    "\n",
    "# TF-IDF feature matrix\n",
    "tfidf = tfidf_vectorizer.fit_transform(dataset['processed_tweets'] )\n",
    "tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Using TFIDF without additional features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you don't specify the random_state in the code, \n",
    "# then every time you run(execute) your code a new random value is generated \n",
    "# and the train and test datasets would have different values each time.\n",
    "X = tfidf\n",
    "y = dataset['class'].astype(int)\n",
    "X_train_tfidf, X_test_tfidf, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.2)\n",
    "model = LogisticRegression().fit(X_train_tfidf,y_train)\n",
    "y_preds = model.predict(X_test_tfidf)\n",
    "report = classification_report( y_test, y_preds )\n",
    "print(report)\n",
    "acc=accuracy_score(y_test,y_preds)\n",
    "print(\"Logistic Regression, Accuracy Score:\" , acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix = confusion_matrix(y_test,y_preds)\n",
    "matrix_proportions = np.zeros((3,3))\n",
    "for i in range(0,3):\n",
    "    matrix_proportions[i,:] = confusion_matrix[i,:]/float(confusion_matrix[i,:].sum())\n",
    "names=['Hate','Offensive','Neither']\n",
    "confusion_df = panda.DataFrame(matrix_proportions, index=names,columns=names)\n",
    "plt.figure(figsize=(5,5))\n",
    "seaborn.heatmap(confusion_df,annot=True,annot_kws={\"size\": 12},cmap='YlGnBu',cbar=False, square=True,fmt='.2f')\n",
    "plt.ylabel(r'True Value',fontsize=14)\n",
    "plt.xlabel(r'Predicted Value',fontsize=14)\n",
    "plt.tick_params(labelsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tfidf, X_test_tfidf, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.2)\n",
    "rf=RandomForestClassifier()\n",
    "rf.fit(X_train_tfidf,y_train)\n",
    "y_preds = rf.predict(X_test_tfidf)\n",
    "acc1=accuracy_score(y_test,y_preds)\n",
    "report = classification_report( y_test, y_preds )\n",
    "print(report)\n",
    "print(\"Random Forest, Accuracy Score:\",acc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix = confusion_matrix(y_test,y_preds)\n",
    "matrix_proportions = np.zeros((3,3))\n",
    "for i in range(0,3):\n",
    "    matrix_proportions[i,:] = confusion_matrix[i,:]/float(confusion_matrix[i,:].sum())\n",
    "names=['Hate','Offensive','Neither']\n",
    "confusion_df = panda.DataFrame(matrix_proportions, index=names,columns=names)\n",
    "plt.figure(figsize=(5,5))\n",
    "seaborn.heatmap(confusion_df,annot=True,annot_kws={\"size\": 12},cmap='YlGnBu',cbar=False, square=True,fmt='.2f')\n",
    "plt.ylabel(r'True Value',fontsize=14)\n",
    "plt.xlabel(r'Predicted Value',fontsize=14)\n",
    "plt.tick_params(labelsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tfidf, X_test_tfidf, y_train, y_test = train_test_split(X.toarray(), y, random_state=42, test_size=0.2)\n",
    "nb=GaussianNB()\n",
    "nb.fit(X_train_tfidf,y_train)\n",
    "y_preds = nb.predict(X_test_tfidf)\n",
    "acc2=accuracy_score(y_test,y_preds)\n",
    "report = classification_report( y_test, y_preds )\n",
    "print(report)\n",
    "print(\"Naive Bayes, Accuracy Score:\",acc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "support =LinearSVC(random_state=20)\n",
    "support.fit(X_train_tfidf,y_train)\n",
    "y_preds = support.predict(X_test_tfidf)\n",
    "acc3=accuracy_score(y_test,y_preds)\n",
    "report = classification_report( y_test, y_preds )\n",
    "print(report)\n",
    "print(\"SVM, Accuracy Score:\" , acc3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using polarity scores as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_analyzer = VS()\n",
    "def count_tags(tweet_c):  \n",
    "    \n",
    "    space_pattern = '\\s+'\n",
    "    giant_url_regex = ('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|'\n",
    "        '[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    mention_regex = '@[\\w\\-]+'\n",
    "    hashtag_regex = '#[\\w\\-]+'\n",
    "    parsed_text = re.sub(space_pattern, ' ', tweet_c)\n",
    "    parsed_text = re.sub(giant_url_regex, 'URLHERE', parsed_text)\n",
    "    parsed_text = re.sub(mention_regex, 'MENTIONHERE', parsed_text)\n",
    "    parsed_text = re.sub(hashtag_regex, 'HASHTAGHERE', parsed_text)\n",
    "    return(parsed_text.count('URLHERE'),parsed_text.count('MENTIONHERE'),parsed_text.count('HASHTAGHERE'))\n",
    "\n",
    "def sentiment_analysis(tweet):   \n",
    "    sentiment = sentiment_analyzer.polarity_scores(tweet)    \n",
    "    twitter_objs = count_tags(tweet)\n",
    "    features = [sentiment['neg'], sentiment['pos'], sentiment['neu'], sentiment['compound'],twitter_objs[0], twitter_objs[1],\n",
    "                twitter_objs[2]]\n",
    "    #features = pandas.DataFrame(features)\n",
    "    return features\n",
    "\n",
    "def sentiment_analysis_array(tweets):\n",
    "    features=[]\n",
    "    for t in tweets:\n",
    "        features.append(sentiment_analysis(t))\n",
    "    return np.array(features)\n",
    "\n",
    "final_features = sentiment_analysis_array(tweet)\n",
    "#final_features\n",
    "\n",
    "new_features = panda.DataFrame({'Neg':final_features[:,0],'Pos':final_features[:,1],'Neu':final_features[:,2],'Compound':final_features[:,3],\n",
    "                            'url_tag':final_features[:,4],'mention_tag':final_features[:,5],'hash_tag':final_features[:,6]})\n",
    "new_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conctaenation of tf-idf scores and sentiment scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_a = tfidf.toarray()\n",
    "modelling_features = np.concatenate([tfidf_a,final_features],axis=1)\n",
    "modelling_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TFIDF with features from sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = panda.DataFrame(modelling_features)\n",
    "y = dataset['class'].astype(int)\n",
    "X_train_bow, X_test_bow, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.2)\n",
    "\n",
    "model = LogisticRegression().fit(X_train_bow,y_train)\n",
    "y_preds = model.predict(X_test_bow)\n",
    "report = classification_report( y_test, y_preds )\n",
    "print(report)\n",
    "acc=accuracy_score(y_test,y_preds)\n",
    "print(\"Logistic Regression,Accuracy Score:\" , acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = panda.DataFrame(modelling_features)\n",
    "y = dataset['class'].astype(int)\n",
    "X_train_bow, X_test_bow, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.2)\n",
    "rf=RandomForestClassifier()\n",
    "rf.fit(X_train_bow,y_train)\n",
    "y_preds = rf.predict(X_test_bow)\n",
    "acc1=accuracy_score(y_test,y_preds)\n",
    "report = classification_report( y_test, y_preds )\n",
    "print(report)\n",
    "print(\"Random Forest, Accuracy Score:\",acc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = panda.DataFrame(modelling_features)\n",
    "y = dataset['class'].astype(int)\n",
    "X_train_bow, X_test_bow, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.2)\n",
    "nb=GaussianNB()\n",
    "nb.fit(X_train_bow,y_train)\n",
    "y_preds = nb.predict(X_test_bow)\n",
    "acc2=accuracy_score(y_test,y_preds)\n",
    "report = classification_report( y_test, y_preds )\n",
    "print(report)\n",
    "print(\"Naive Bayes, Accuracy Score:\",acc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = panda.DataFrame(modelling_features)\n",
    "y = dataset['class'].astype(int)\n",
    "X_train_bow, X_test_bow, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.2)\n",
    "support =LinearSVC(random_state=20)\n",
    "support.fit(X_train_bow,y_train)\n",
    "y_preds = support.predict(X_test_bow)\n",
    "acc3=accuracy_score(y_test,y_preds)\n",
    "report = classification_report( y_test, y_preds )\n",
    "print(report)\n",
    "print(\"SVM, Accuracy Score:\" , acc3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create doc2vec vector columns\n",
    "# Initialize and train the model\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "#The input for a Doc2Vec model should be a list of TaggedDocument(['list','of','word'], [TAG_001]). \n",
    "#A good practice is using the indexes of sentences as the tags.\n",
    "documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(dataset[\"processed_tweets\"].apply(lambda x: x.split(\" \")))]\n",
    "\n",
    "# train a Doc2Vec model with our text data\n",
    "# window- The maximum distance between the current and predicted word within a sentence.\n",
    "# mincount-Ignores all words with total frequency lower than this.\n",
    "# workers -Use these many worker threads to train the model\n",
    "#  Training Model - distributed bag of words (PV-DBOW) is employed.\n",
    "model = Doc2Vec(documents,vector_size=5, window=2, min_count=1, workers=4)\n",
    "\n",
    "#infer_vector - Infer a vector for given post-bulk training document.\n",
    "# Syntax- infer_vector(doc_words, alpha=None, min_alpha=None, epochs=None, steps=None)\n",
    "# doc_words-A document for which the vector representation will be inferred.\n",
    "\n",
    "# transform each document into a vector data\n",
    "doc2vec_df = dataset[\"processed_tweets\"].apply(lambda x: model.infer_vector(x.split(\" \"))).apply(panda.Series)\n",
    "doc2vec_df.columns = [\"doc2vec_vector_\" + str(x) for x in doc2vec_df.columns]\n",
    "doc2vec_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelling_features = np.concatenate([tfidf_a,final_features,doc2vec_df],axis=1)\n",
    "modelling_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = panda.DataFrame(modelling_features)\n",
    "y = dataset['class'].astype(int)\n",
    "X_train_bow, X_test_bow, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.2)\n",
    "\n",
    "model = LogisticRegression().fit(X_train_bow,y_train)\n",
    "y_preds = model.predict(X_test_bow)\n",
    "report = classification_report( y_test, y_preds )\n",
    "print(report)\n",
    "acc=accuracy_score(y_test,y_preds)\n",
    "print(\"Logistic Regression, Accuracy Score:\" , acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = panda.DataFrame(modelling_features)\n",
    "y = dataset['class'].astype(int)\n",
    "X_train_bow, X_test_bow, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.2)\n",
    "rf=RandomForestClassifier()\n",
    "rf.fit(X_train_bow,y_train)\n",
    "y_preds = rf.predict(X_test_bow)\n",
    "acc1=accuracy_score(y_test,y_preds)\n",
    "report = classification_report( y_test, y_preds )\n",
    "print(report)\n",
    "print(\"Random Forest, Accuracy Score:\",acc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = panda.DataFrame(modelling_features)\n",
    "y = dataset['class'].astype(int)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.2)\n",
    "nb=GaussianNB()\n",
    "nb.fit(X_train,y_train)\n",
    "y_preds = nb.predict(X_test)\n",
    "acc2=accuracy_score(y_test,y_preds)\n",
    "report = classification_report( y_test, y_preds )\n",
    "print(report)\n",
    "print(\"Naive Bayes, Accuracy Score:\",acc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = panda.DataFrame(modelling_features)\n",
    "y = dataset['class'].astype(int)\n",
    "X_train_bow, X_test_bow, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.2)\n",
    "support =LinearSVC(random_state=20)\n",
    "support.fit(X_train_bow,y_train)\n",
    "y_preds = support.predict(X_test_bow)\n",
    "acc3=accuracy_score(y_test,y_preds)\n",
    "report = classification_report( y_test, y_preds )\n",
    "print(report)\n",
    "print(\"SVM, Accuracy Score:\" , acc3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(learner_list, X_train, y_train, X_test, y_test): \n",
    "    '''\n",
    "    inputs:\n",
    "       - learner: the learning algorithm to be trained and predicted on\n",
    "       - X_train: features training set\n",
    "       - y_train: income training set\n",
    "       - X_test: features testing set\n",
    "       - y_test: income testing set\n",
    "    '''\n",
    "    \n",
    "    # Get length of Training Data:\n",
    "    size = len(y_train)\n",
    "    \n",
    "    results = {}\n",
    "    final_results = []\n",
    "    \n",
    "    for learner in learner_list:\n",
    "        \n",
    "        # Store the learner name:\n",
    "        results['Algorithm'] = learner.__class__.__name__\n",
    "\n",
    "        # Fit the learner:\n",
    "        start = time() # Get start time\n",
    "        print(\"Training {}\".format(learner.__class__.__name__))\n",
    "        learner = learner.fit(X_train, y_train)\n",
    "        end = time() # Get end time\n",
    "\n",
    "        # Store the training time\n",
    "        results['Training Time'] = end - start\n",
    "\n",
    "        start = time() # Get start time\n",
    "        predictions_test = learner.predict(X_test)\n",
    "        predictions_train = learner.predict(X_train)\n",
    "        end = time() # Get end time\n",
    "\n",
    "        # Store the prediction time\n",
    "        results['Prediction Time'] = end - start\n",
    "\n",
    "        # Compute the Accuracy on Test Set\n",
    "        results['Accuracy: Test'] = accuracy_score(y_test, predictions_test)\n",
    "\n",
    "        # Compute the Accuracy on Training Set\n",
    "        results['Accuracy: Train'] = accuracy_score(y_train, predictions_train)\n",
    "\n",
    "        # Compute the F1 Score on Test Set\n",
    "        results['F1 Score: Test'] = f1_score(y_test, predictions_test)\n",
    "\n",
    "        # Compute the F1 Score on Training Set\n",
    "        results['F1 Score: Train'] = f1_score(y_train, predictions_train)\n",
    "\n",
    "        # Compute the Precision on Test Set\n",
    "        results['Precision: Test'] = precision_score(y_test, predictions_test)\n",
    "\n",
    "        # Compute the Precision on Training Set\n",
    "        results['Precision: Train'] = precision_score(y_train, predictions_train)\n",
    "\n",
    "        # Compute the Recall on Test Set\n",
    "        results['Recall: Test'] = recall_score(y_test, predictions_test)\n",
    "\n",
    "        # Compute the Recall on Training Set\n",
    "        results['Recall: Train'] = recall_score(y_train, predictions_train)\n",
    "\n",
    "        # Success\n",
    "        print(\"Training {} finished in {:.2f} sec\".format(learner.__class__.__name__, results['Training Time']))\n",
    "        print('----------------------------------------------------')\n",
    "        \n",
    "        final_results.append(results.copy())\n",
    "    # Return a dataframe of the results\n",
    "    return final_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [MultinomialNB(), DecisionTreeClassifier(), LinearSVC(), AdaBoostClassifier(), \n",
    "          RandomForestClassifier(), BaggingClassifier(),\n",
    "         LogisticRegression(), SGDClassifier(), KNeighborsClassifier()]\n",
    "\n",
    "re = pipeline(models, training_data, y_train, testing_data, y_test)\n",
    "results = pd.DataFrame(re)\n",
    "results = results.reindex(columns = ['Algorithm', 'Accuracy: Test', 'Precision: Test', 'Recall: Test', 'F1 Score: Test', 'Prediction Time',\n",
    "                          'Accuracy: Train', 'Precision: Train', 'Recall: Train', 'F1 Score: Train', 'Training Time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = results.reindex(columns = ['Algorithm', 'Accuracy: Test', 'Precision: Test', 'Recall: Test', 'F1 Score: Test', 'Prediction Time',\n",
    "                          'Accuracy: Train', 'Precision: Train', 'Recall: Train', 'F1 Score: Train', 'Training Time'])\n",
    "\n",
    "results.sort_values(by = 'F1 Score: Test', inplace = True, ascending = False)\n",
    "results.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_acc = results[results['Accuracy: Test'] == results['Accuracy: Test'].max()]\n",
    "best_f1 = results[results['F1 Score: Test'] == results['F1 Score: Test'].max()]\n",
    "best_precision = results[results['Precision: Test'] == results['Precision: Test'].max()]\n",
    "best_recall = results[results['Recall: Test'] == results['Recall: Test'].max()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('darkgrid')\n",
    "plt.figure(figsize = (25, 10))\n",
    "\n",
    "barWidth = 0.17\n",
    " \n",
    "# set height of bar\n",
    "bars1 = results['Accuracy: Test']\n",
    "bars2 = results['F1 Score: Test']\n",
    "bars3 = results['Precision: Test']\n",
    "bars4 = results['Recall: Test']\n",
    "\n",
    " \n",
    "# Set position of bar on X axis\n",
    "r1 = np.arange(len(bars1))\n",
    "r2 = [x + barWidth for x in r1]\n",
    "r3 = [x + barWidth for x in r2]\n",
    "r4 = [x + barWidth for x in r3]\n",
    "\n",
    " \n",
    "# Make the plot\n",
    "pal = sns.color_palette()\n",
    "plt.bar(r1, bars1, color= pal[0], width=barWidth, edgecolor='white', label='Test Accuracy')\n",
    "plt.bar(r2, bars2, color= pal[1], width=barWidth, edgecolor='white', label='F1 Score')\n",
    "plt.bar(r3, bars3, color= pal[2], width=barWidth, edgecolor='white', label='Precision')\n",
    "plt.bar(r4, bars4, color= pal[4], width=barWidth, edgecolor='white', label='Recall')\n",
    "\n",
    " \n",
    "# Add xticks on the middle of the group bars\n",
    "plt.xlabel('Algorithm', fontweight='bold', fontsize = 13)\n",
    "plt.ylabel('Score', fontweight = 'bold', fontsize = 13)\n",
    "plt.xticks([r + barWidth for r in range(len(bars1))], results['Algorithm'], rotation = 15, fontsize = 11)\n",
    " \n",
    "# Create legend & Show graphic\n",
    "plt.legend(fontsize = 13)\n",
    "\n",
    "#textstr = '\\n'.join(['Best Accuracy: {:.3f} - {}'.format(best_acc['Accuracy: Test'].values[0], best_acc['Algorithm'].values[0]), \n",
    "#                     'Best F1 Score: {:.3f} - {}'.format(best_f1['F1 Score: Test'].values[0], best_f1['Algorithm'].values[0]),\n",
    "#                   'Best Precision: {:.3f} - {}'.format(best_precision['Precision: Test'].values[0], best_precision['Algorithm'].values[0]), \n",
    "#                    'Best Recall: {:.3f} - {}'.format(best_recall['Recall: Test'].values[0], best_recall['Algorithm'].values[0])])\n",
    "props = dict(boxstyle='round', facecolor='lightgrey', alpha=0.5)\n",
    "\n",
    "#place a text box\n",
    "#plt.text(9.2, 1, textstr, fontsize=14, verticalalignment='top', bbox=props)\n",
    "\n",
    "plt.title('Classification Summary of Algorithms', fontweight = 'bold', fontsize = 17);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 12\n",
    "batch_size = 512\n",
    "\n",
    "\n",
    "history = model.fit(x_train, y_tr, epochs=num_epochs, validation_split=0.1, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(cnnTestData, batch_size=1024, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['Hate', 'Offensive', 'Neither']\n",
    "\n",
    "def makeprediction(texts):\n",
    "    tk = Tokenizer()\n",
    "    tk.fit_on_texts(texts)\n",
    "    index_list = tk.texts_to_sequences(texts)\n",
    "    x_train = pad_sequences(index_list, maxlen=50)\n",
    "    pred = model.predict(x_train)\n",
    "    result = labels[np.argmax(pred)]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is OFFENSIVE\n",
    "texts = ['Kick out all Bangladesh people from bengal and kick out Manama too']\n",
    "print(makeprediction(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_labels=[]\n",
    "for p in predictions:\n",
    "    prediction_labels.append(labels[np.argmax(p)])\n",
    "\n",
    "sum(data_test.Label==prediction_labels)/len(prediction_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test.Label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1)\n",
    "# summarize history for accuracy\n",
    "plt.subplot(211)\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "# summarize history for loss\n",
    "plt.subplot(212)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = confusion_matrix(data_test.Label, prediction_labels,normalize=True)\n",
    "print ('Confusion Matrix :')\n",
    "print(results) \n",
    "print ('Accuracy Score :',accuracy_score(data_test.Label, prediction_labels))\n",
    "print ('Report : ')\n",
    "print (classification_report(data_test.Label, prediction_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_path = 'GoogleNews-vectors-negative300.bin'\n",
    "#word2vec_path = 'glove.6B.50d.word2vec'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(13, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ConvolutionalNeuralNet(embeddings, max_sequence_length, num_words, embedding_dim, labels_index):\n",
    "    \n",
    "    # Embedding layer\n",
    "    embedding_layer = Embedding(num_words,\n",
    "                            embedding_dim,\n",
    "                            weights=[embeddings],\n",
    "                            input_length=max_sequence_length,\n",
    "                            trainable=False)\n",
    "    \n",
    "    sequence_input = Input(shape=(max_sequence_length,), dtype='int32')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "    convs = []\n",
    "    \n",
    "    #Kernal (window) size\n",
    "    filter_sizes = [2,3,4,5]\n",
    "\n",
    "    # Convolution Layer and max-pooling\n",
    "    for filter_size in filter_sizes:\n",
    "        l_conv = Conv1D(filters=512, kernel_size=filter_size, activation='relu')(embedded_sequences)\n",
    "        l_pool = GlobalMaxPooling1D()(l_conv)\n",
    "        convs.append(l_pool)\n",
    "        \n",
    "    l_merge = concatenate(convs, axis=1)\n",
    "    \n",
    "    # softmax layer for multiclass identification \n",
    "    x = Dropout(0.1)(l_merge)  \n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    preds = Dense(labels_index, activation='softmax')(x)\n",
    "\n",
    "    model = Model(sequence_input, preds)\n",
    "    model.compile(loss= 'categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "   \n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvolutionalNeuralNet(train_embedding_weights, MAX_SEQUENCE_LENGTH, len(train_word_index)+1, EMBEDDING_DIM, len(list(label_names)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 12\n",
    "batch_size = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x_train, y_tr, epochs=num_epochs, validation_split=0.1, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(cnnTestData, batch_size=1024, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['Hate', 'Offensive', 'Neither']\n",
    "\n",
    "def makeprediction(texts):\n",
    "    tk = Tokenizer()\n",
    "    tk.fit_on_texts(texts)\n",
    "    index_list = tk.texts_to_sequences(texts)\n",
    "    x_train = pad_sequences(index_list, maxlen=50)\n",
    "    pred = model.predict(x_train)\n",
    "    result = labels[np.argmax(pred)]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = ['Kick out all Bangladesh people from bengal and kick out Manama too']\n",
    "print(makeprediction(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_labels=[]\n",
    "for p in predictions:\n",
    "    prediction_labels.append(labels[np.argmax(p)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(data_test.Label==prediction_labels)/len(prediction_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test.Label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1)\n",
    "# summarize history for accuracy\n",
    "plt.subplot(211)\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "# summarize history for loss\n",
    "plt.subplot(212)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "print(\"Convolution Neural Network\")\n",
    "#conf_mat = confusion_matrix(data_test.Label, prediction_labels)\n",
    "#fig, ax = plt.subplots(figsize=(5,5))\n",
    "#sns.heatmap(conf_mat, annot=True, fmt='d', cmap ='YlGnBu',\n",
    "#           xticklabels=category_id_df.Label.values, yticklabels=category_id_df.Label.values)#plt.ylabel('Actual')\n",
    "#plt.xlabel('Predicted')\n",
    "#plt.show()\n",
    "\n",
    "confusion_matrix = confusion_matrix(data_test.Label, prediction_labels)\n",
    "matrix_proportions = np.zeros((3,3))\n",
    "for i in range(0,3):\n",
    "    matrix_proportions[i,:] = confusion_matrix[i,:]/float(confusion_matrix[i,:].sum())\n",
    "names=['Hate','Offensive','Neither']\n",
    "confusion_df = panda.DataFrame(matrix_proportions, index=names,columns=names)\n",
    "plt.figure(figsize=(5,5))\n",
    "seaborn.heatmap(confusion_df,annot=True,annot_kws={\"size\": 12},cmap='YlGnBu',cbar=False, square=True,fmt='.2f')\n",
    "plt.ylabel(r'True Value',fontsize=14)\n",
    "plt.xlabel(r'Predicted Value',fontsize=14)\n",
    "plt.tick_params(labelsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
